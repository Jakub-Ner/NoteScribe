{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNYRmRMkqJmL",
        "outputId": "0bd24fc3-17c8-4d13-e204-e885df23530c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n",
            "11_LABS_API_KEY: ··········\n"
          ]
        }
      ],
      "source": [
        "#@title Initial configuration\n",
        "!pip install langchain openai elevenlabs > /dev/null 2>&1\n",
        "!pip install pytesseract > /dev/null 2>&1\n",
        "!pip install pdf2image > /dev/null 2>&1\n",
        "!pip install fpdf > /dev/null 2>&1\n",
        "\n",
        "!pip install unstructured tiktoken chromadb chroma pypdf > /dev/null 2>&1\n",
        "!apt-get install tesseract-ocr > /dev/null 2>&1\n",
        "!apt-get install libtesseract-dev > /dev/null 2>&1\n",
        "!apt-get install poppler-utils > /dev/null 2>&1\n",
        "\n",
        "# Download Polish language data file\n",
        "!wget https://github.com/tesseract-ocr/tessdata/raw/main/pol.traineddata > /dev/null 2>&1\n",
        "\n",
        "# Specify the Tesseract data directory\n",
        "tessdata_dir = '/usr/share/tesseract-ocr/4.00/tessdata/'\n",
        "\n",
        "# Move the downloaded language data file to the Tesseract data directory\n",
        "!mv pol.traineddata $tessdata_dir\n",
        "\n",
        "# Set TESSDATA_PREFIX environment variable\n",
        "import os\n",
        "import elevenlabs\n",
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass(\"OPENAI_API_KEY: \")\n",
        "elevenlabs.set_api_key(getpass(\"11_LABS_API_KEY: \")) \n",
        "\n",
        "os.environ['TESSDATA_PREFIX'] = tessdata_dir\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Utils**"
      ],
      "metadata": {
        "id": "dLZ4MdtbfNQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Convert .md -> .pdf { display-mode: \"form\" }\n",
        "!sudo apt-get install pandoc texlive-latex-base texlive-fonts-recommended texlive-extra-utils texlive-latex-extra > /dev/null 2>&1\n",
        "\n",
        "def md_to_pdf():\n",
        "  \"\"\"Converts md file into pdf.\"\"\"\n",
        "  !pandoc /content/sample_data/conversation.md -o /content/sample_data/conversation.pdf\n"
      ],
      "metadata": {
        "id": "SB91ot3rqhd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Downloader { display-mode: \"form\" }\n",
        "from google.colab import files\n",
        "\n",
        "def download_conversation(text, file_path=\"/content/sample_data/conversation\"):\n",
        "  with open(f\"{file_path}.md\", \"w+\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "  md_to_pdf()\n",
        "  files.download(f\"{file_path}.pdf\")\n"
      ],
      "metadata": {
        "id": "LeE7pxHdkn1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print Markdown { display-mode: \"form\" }\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def print_md(text=r'#**Hello** *World:* $\\frac{1}{2} \\times \\pi \\times r^2$'):\n",
        "    display(Markdown(text))\n",
        "print_md()"
      ],
      "metadata": {
        "id": "AIbzkQtmfZUI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "488faa08-62ec-4808-f71b-b6f410af7358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#**Hello** *World:* $\\frac{1}{2} \\times \\pi \\times r^2$"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 11Labs { display-mode: \"form\" }\n",
        "from elevenlabs import generate, play\n",
        "\n",
        "def generate_and_play(text=\"Some very long text to be read by the voice\"):\n",
        "  audio = generate(\n",
        "      text=text, \n",
        "      model='eleven_multilingual_v1'\n",
        "      )\n",
        "\n",
        "  play(audio, notebook=True)\n"
      ],
      "metadata": {
        "id": "UICnmBg-fZYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agent Tools**"
      ],
      "metadata": {
        "id": "MyM-xo-Hq7I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Menu View\n",
        "MENU_VIEW = \\\n",
        "\"\"\"# Welcome!\n",
        "## Settings:\n",
        " - audio (**on**/off)\n",
        " - text formatting (**.md**/.txt)\n",
        " - language\n",
        "\n",
        "## Avaliable programs:\n",
        " - compendium - university courses content scraper and personal tutor\n",
        " - img2pdf - converter for images to pdfd\n",
        "\"\"\"\n",
        "\n",
        "MENU_VIEW_PL = \\\n",
        "\"\"\"# Witaj\n",
        "## Ustawienia:\n",
        " - audio (**wł** / wył)\n",
        " - formatowanie tekstu (**.md** / .txt)\n",
        " - język\n",
        "\n",
        "## Dostępne narzędzia:\n",
        " - Compendium -  Wyszukiwarka wiedzy uniwersyteckiej i osobisty tutor\n",
        " - im2pdf - Konwerter zdjęć do pdf\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vKQJ5w73n2Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ChatConfiguration { display-mode: \"form\" }\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.tools import tool\n",
        "\n",
        "USE_CASE_LITERAL = Literal[\"menu\", \"compendium\"]\n",
        "STATE_AUDIO_LITERAL = Literal[\"on\", \"off\"]\n",
        "STATE_TEXT_LITERAL = Literal[\"md\", \"txt\"]\n",
        "\n",
        "class ChatConfigurationSingleton:\n",
        "  __instance = None\n",
        "\n",
        "  use_case: USE_CASE_LITERAL = \"menu\"\n",
        "  audio_state: STATE_AUDIO_LITERAL = \"on\"\n",
        "  text_state: STATE_TEXT_LITERAL = \"md\"\n",
        "\n",
        "  used_tokens = 0\n",
        "  total_cost = 0\n",
        "\n",
        "  def __new__(cls, *args, **kwargs):\n",
        "    if not cls.__instance:\n",
        "        cls.__instance = super().__new__(cls, *args, **kwargs)\n",
        "    return cls.__instance\n",
        "\n",
        "  def extract_inf(self, openai_cb): # BUG\n",
        "    self.used_tokens += openai_cb.total_tokens\n",
        "    self.total_cost += openai_cb.total_cost\n",
        "\n",
        "  def show(self, text):\n",
        "    if self.text_state == \"md\":\n",
        "      print_md(text)\n",
        "    else:\n",
        "      print(text)\n",
        "\n",
        "    if self.audio_state == \"on\":\n",
        "      generate_and_play(text)\n",
        "      \n",
        "\n",
        "  def run(self, run_func, text):\n",
        "    with get_openai_callback() as cb:\n",
        "      response = run_func(text)\n",
        "      self.extract_inf(cb) # BUG\n",
        "\n",
        "    self.show(response)\n",
        "\n",
        "chat_config = ChatConfigurationSingleton()\n",
        "\n",
        "\n",
        "@tool(return_direct=True)\n",
        "def set_use_case_toool(state: USE_CASE_LITERAL) -> str:\n",
        "  \"\"\"Useful at the beginning, when user knows what he want\n",
        "    @param use_case: The use case to be processed.\n",
        "      \"menu\" - back to menu\n",
        "      \"compendium\" - university courses content scraper and personal tutor\n",
        "      img2pdf\n",
        "  \"\"\"\n",
        "  chat_config.use_case = state\n",
        "  return f\"Switched to {state}\"\n",
        "\n",
        "@tool(return_direct=True)\n",
        "def set_audio_tool(state: STATE_AUDIO_LITERAL) -> str:\n",
        "  \"\"\"Useful when user want or don't want speaking responses\"\"\"\n",
        "  chat_config.audio_state = state\n",
        "  return f\"From now audio responses are turned {state}\"\n",
        "\n",
        "@tool(return_direct=True)\n",
        "def set_markdown_tool(state: STATE_TEXT_LITERAL) -> str:\n",
        "  \"\"\"Turn off or on markdown formatted responses\"\"\"\n",
        "  chat_config.text_state = state\n",
        "  return f\"From now messages are displayed using {state}\"\n"
      ],
      "metadata": {
        "id": "Y5MKE93eqv3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d11c086-c806-412f-df20-53456a260925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.utilities.powerbi:Could not import azure.core python package.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Compendium**"
      ],
      "metadata": {
        "id": "5R_AU4Qlte4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "courses_recources = {\n",
        "    \"Fizyka I\" : [\n",
        "      (\"pl\", \"/content/GFG-15.pdf\")   \n",
        "    ],\n",
        "    \"analiza matematyczna\": {\n",
        "        \n",
        "    }\n",
        "} "
      ],
      "metadata": {
        "id": "DSe_CZ-Lxb97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Resource fetcher\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "import json\n",
        "\n",
        "def getLanguage(text):\n",
        "  llm = OpenAI(temperature = 0)\n",
        "\n",
        "  prompt = PromptTemplate(\n",
        "          input_variables=[\"text\"],\n",
        "          template=\"What is a language of that text: {text}? Answer in one word\",\n",
        "      )\n",
        "\n",
        "  llm_chain = LLMChain(prompt=prompt, llm=OpenAI())\n",
        "  return llm_chain.run(text).replace(\"\\n\", \"\").replace(\":\", \"\")\n",
        "\n",
        "\n",
        "def addDataSource(name_of_course: str, pdf_path: str):\n",
        "\n",
        "  loader = UnstructuredPDFLoader(pdf_path)\n",
        "\n",
        "  data = loader.load()\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=0)\n",
        "  texts = text_splitter.split_documents(data)\n",
        "\n",
        "  data_source = (getLanguage(texts[0]), pdf_path)\n",
        "\n",
        "  try:\n",
        "    with open(\"/content/embeddings.json\", \"r\") as file:\n",
        "      data = json.load(file)\n",
        "  except FileNotFoundError:\n",
        "    data = {}\n",
        "\n",
        "  if name_of_course in data:\n",
        "      data[name_of_course].append(data_source)\n",
        "  else:\n",
        "      data[name_of_course] = [data_source]\n",
        "\n",
        "\n",
        "  with open(\"/content/embeddings.json\", \"w\") as outfile:\n",
        "    json.dump(data, outfile)\n",
        "    \n"
      ],
      "metadata": {
        "id": "41PBlP_F9_s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title helpers { display-mode: \"form\" }\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def get_questions(zakres, lang):\n",
        "  match lang:\n",
        "    case \"pl\" : template_questions = \"Zadaj pytania dotyczace badz zwiazane z tym zagadnieniem: {zakres}, wylistuj je i zakoncz dzialanie. Pytania:\"\n",
        "    case \"eng\" : template_questions = \"Ask question about or related to topic {zakres}. List them and end task. Questions:\" # BUG\n",
        "\n",
        "  llm_questions = ChatOpenAI(temperature=0.8, max_tokens=1000)\n",
        "  prompt_questions = PromptTemplate(input_variables=[\"zakres\"], template=template_questions)\n",
        "  chain_questions = LLMChain(llm=llm_questions,prompt=prompt_questions)\n",
        "  return chain_questions({\"zakres\":zakres}, return_only_outputs=True)['text'].split(\"\\n\")[:-2] # -2 JOŁ ZMIEN TO PRZED \n",
        "\n",
        "\n",
        "def get_resources(nazwa_kursu):\n",
        "  lang, url = courses_recources[nazwa_kursu][0]\n",
        "\n",
        "  match lang:\n",
        "    case \"pl\": print_md(f\"Użyję {url}\")\n",
        "    case \"eng\": print_md(f\"I will use {url}\")\n",
        "\n",
        "  return lang, url\n",
        "\n",
        "\n",
        "def get_documents(url):\n",
        "  loader = PyPDFLoader(url)\n",
        "  pages = loader.load_and_split()\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
        "  return text_splitter.split_documents(pages)\n",
        "\n",
        "def get_QA(summary, questions_list):\n",
        "  llm = ChatOpenAI(temperature=0.8,max_tokens=1000, model_name=\"gpt-3.5-turbo\")\n",
        "  promptTester = PromptTemplate(\n",
        "      input_variables = [\"summary\", \"QA\"],\n",
        "      template = \"\"\"Create a test to help memorize and practice the material from {summary} and {QA}. There should be multiple questions\n",
        "      The format should be:\n",
        "      [QUESTION]?\n",
        "      a) answer\n",
        "      b) answer\n",
        "      c) answer\n",
        "      d) answer\n",
        "\n",
        "      Odpowiedź:\n",
        "      \"\"\",\n",
        "  )\n",
        "\n",
        "  testerChain = LLMChain(llm=llm, prompt=promptTester, output_key=\"test\")\n",
        "  return testerChain({\"summary\":summary,\"QA\":questions_list})[\"test\"]\n",
        "\n",
        "\n",
        "def get_notes(summary, QA):\n",
        "  llm = OpenAI(temperature=0.8,max_tokens=1000, model_name=\"gpt-3.5-turbo\")\n",
        "  promptNote = PromptTemplate(\n",
        "      input_variables = [\"sumText\",\"QA\" ],\n",
        "      template = \"\"\"Connect the information from {sumText} and {QA} into a big note from which students can learn and remove breaklines.\"\"\",\n",
        "  )\n",
        "\n",
        "  noteChain = LLMChain(llm=llm,prompt=promptNote, output_key=\"summary\")\n",
        "  return noteChain({\"sumText\": summary,\"QA\": QA})[\"summary\"]"
      ],
      "metadata": {
        "id": "5h2RvRxAtxFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "si1SYqMIgUi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Compendium\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain import VectorDBQA\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "def ask(msg, content):\n",
        "  option = input(msg)\n",
        "  # content += f\"{msg}:{option}\"\n",
        "  return option == \"yes\" or option ==  \"tak\" or option ==  \"pewnie\" or option ==  \"sure\" \n",
        "    \n",
        "def run_compendium(nazwa_kursu, zakres, url, lang):\n",
        "  with get_openai_callback() as cb:\n",
        "    if not (url and lang):\n",
        "      lang, url = get_resources(nazwa_kursu)\n",
        "    questions_list = get_questions(zakres, lang)\n",
        "\n",
        "    for question in questions_list:\n",
        "      chat_config.show(question)\n",
        "\n",
        "    document = get_documents(url)\n",
        "    # print(f'Now you have {len(document)} documents')\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "    docsearch = Chroma.from_documents(document, embeddings)\n",
        "    chain = RetrievalQA.from_chain_type(llm=ChatOpenAI(temperature=0.7,max_tokens=1000) ,chain_type='map_reduce', retriever=docsearch.as_retriever(), return_source_documents=True)\n",
        "\n",
        "    summary = \"\"\n",
        "    for question in questions_list:\n",
        "      summary += chain({'query':f'{question} Rozwin swoja wypowiedz o jak najwiecej szczegolow'},return_only_outputs=True)['result']\n",
        "    \n",
        "    content = f\"## Podsumowanie:\\n{summary}\\n\" \n",
        "    chat_config.show(summary)\n",
        "    \n",
        "    # QA Maker\n",
        "    if ask(\"Chcesz sprawdzić swoją wiedzę?:\", content):\n",
        "      QA = get_QA(summary, questions_list)\n",
        "      content += f\"\\n## Test wiedzy:\\n{QA}\\n\" \n",
        "      chat_config.show(QA)\n",
        "\n",
        "    # Notes Maker\n",
        "    if ask(\"Chcesz notatki?:\", content):\n",
        "      notes = get_notes(summary, QA)\n",
        "      content += f\"\\n## Notatki:\\n{notes}\\n\"\n",
        "      chat_config.show(notes)\n",
        "\n",
        "    content = f\"\\n\\n*Used {chat_config.used_tokens} tokens, what cost: ${chat_config.total_cost}*\"\n",
        "    # Downloader\n",
        "    if ask(\"Chcesz pobrać:\", content):\n",
        "      download_conversation(content)\n",
        "    \n",
        "    chat_config.extract_inf(cb)\n"
      ],
      "metadata": {
        "id": "Apb6nnX4vH_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imgs2PDF**"
      ],
      "metadata": {
        "id": "HOROUp0B9t1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from fpdf import FPDF\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "def delete_special_characters(word):\n",
        "  for character in word:\n",
        "    if ord(character) > 127:\n",
        "      return False\n",
        "  return True\n",
        "    \n",
        "    \n",
        "def convert_images_to_searchable_pdf(pdf_path: str, return_path: str) -> None:\n",
        "    \"\"\"Extracts text from an image and saves it as pdf file to user device\"\"\"\n",
        "    images = convert_from_path(pdf_path)\n",
        "    # save FPDF() class into a variable pdf\n",
        "    pdf = FPDF()\n",
        "    for i, image in enumerate(images):\n",
        "        # Extract text from image\n",
        "        text = pytesseract.image_to_string(image, lang=\"pol\")\n",
        "        # Add a page\n",
        "        pdf.add_page()\n",
        "        \n",
        "        # set style and size of font\n",
        "        # that you want in the pdf\n",
        "        pdf.set_font(\"Arial\", size=15)\n",
        "        \n",
        "        for line in text.split('\\n'):\n",
        "            if delete_special_characters(line):\n",
        "                pdf.cell(200, 10, txt=line, ln=1, align='C')\n",
        "        \n",
        "        # save the pdf with name .pdf\n",
        "    pdf.output(return_path) \n",
        "       \n",
        "\n",
        "pdf_path = '/content/530_Krotki_wyk_ad_z_fizyki_ogolnej.pdf' #@param {type:\"string\"}\n",
        "return_path = '/content/GFG.pdf' #@param {type:\"string\"}\n",
        "# convert_images_to_searchable_pdf(pdf_path)"
      ],
      "metadata": {
        "id": "w4Uu2He990gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Demo**"
      ],
      "metadata": {
        "id": "xMEpHl27qj4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize Agent\n",
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "chat_llm = ChatOpenAI(temperature=0)\n",
        "tools = [set_use_case_toool, set_audio_tool, set_markdown_tool]\n",
        "agent = initialize_agent(tools, chat_llm, agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, memory=memory)\n",
        "\n",
        "print_md(MENU_VIEW_PL)\n",
        "memory.chat_memory.add_ai_message(MENU_VIEW_PL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "TlaKBnKPfYaV",
        "outputId": "8003e142-0c2e-4185-a2fb-fb884a3f5724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Witaj\n## Ustawienia:\n - audio (**wł** / wył)\n - formatowanie tekstu (**.md** / .txt)\n - język\n\n## Dostępne narzędzia:\n - Compendium -  Wyszukiwarka wiedzy uniwersyteckiej i osobisty tutor\n - im2pdf - Konwerter zdjęć do pdf\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Conversation\n",
        "\n",
        "match chat_config.use_case:\n",
        "  case \"menu\":\n",
        "    #@markdown **Menu**\n",
        "    User_input = \"switch to compendium\" #@param {type:\"string\"}\n",
        "    chat_config.run(agent.run, User_input)\n",
        "\n",
        "  case \"compendium\":\n",
        "    #@markdown **Compendium:**\n",
        "    nazwa_kursu = \"Fizyka I\" #@param [\"Fizyka I\", \"Analiza matematyczna\"]\n",
        "    zakres = \"Distributed Alignment Search and Boundless DAS. Their role in Alpaca language models\" #@param {type:\"string\"}\n",
        "    literatura = \"https://arxiv.org/pdf/2305.08809.pdf\" #@param {type:\"string\"}\n",
        "    jezyk_literatury = \"eng\" #@param {type:\"string\"}\n",
        "\n",
        "    run_compendium(nazwa_kursu, zakres, literatura, jezyk_literatury)\n",
        "    chat_config.use_case = \"menu\" # powrot do menu\n",
        "\n",
        "  case \"img2pdf\":\n",
        "    #@markdown **ImgPDF:**\n",
        "    pdf_path = '' #@param {type:\"string\"}\n",
        "    return_path = '' #@param {type:\"string\"}\n",
        "\n",
        "    convert_images_to_searchable_pdf(pdf_path, return_path)\n",
        "    chat_config.use_case = \"menu\" # powrot do menu\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pSYkoUN3fYnU",
        "outputId": "b0128714-a20f-40b0-d158-7de419b2be1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1. What is Distributed Alignment Search and how does it work? "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "2. How does Boundless DAS differ from traditional alignment-based algorithms? "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "3. What role do Distributed Alignment Search and Boundless DAS play in Alpaca language models? "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "4. How do these technologies improve the accuracy of natural language processing (NLP) tasks in Alpaca language models? "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "5. Can Distributed Alignment Search and Boundless DAS be used in other types of NLP applications besides Alpaca language models? "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Task: "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "List the benefits of incorporating Distributed Alignment Search and Boundless DAS into Alpaca language models. "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Answer: "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Increased accuracy in natural language processing tasks"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Enhanced ability to handle complex sentence structures and syntax"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Improved performance on large-scale datasets "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Reduced computational costs and faster processing times "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Distributed Alignment Search (DAS) is a method that aims to align neural structures across multiple neurons or brain regions, language models, or deep learning models with interpretable symbolic algorithms. It works by leveraging the power of distributed computing to search for the optimal alignment between two structures using gradient descent. DAS is a more general approach compared to previous methods which often focus on zeroing-out representations or substituting with mean values.\n\nDAS involves the use of an orthogonal matrix and is implemented through a series of iterative steps. The ultimate goal of DAS is to improve the interpretability of neural networks by aligning the learned representations with the causal variables of the model or with interpretable symbolic algorithms. \n\nThe approach of DAS is different from previous work in the field that usually relies on the assumption of localist representation. DAS does not rely on the assumption of a one-to-one mapping between a group of neurons and a high-level concept. Instead, multiple concepts can align with the same group of neurons, or a group of neurons can map to multiple concepts.\n\nBoundless DAS is a specific type of DAS that focuses on distributed alignment search and breaks the assumption of localist representation. Additionally, it can be adapted into a head-wise alignment search by adding a shared rotation matrix on top of head representations of all the tokens. Overall, DAS is a promising method for aligning neural structures and has the potential to advance our understanding of the brain and improve the interpretability of neural networks.Niestety, nadal potrzebuję bardziej szczegółowych informacji lub konkretne pytanie, aby móc rozwijać swoją wypowiedź. Bez konkretnego tematu lub kontekstu, nie mam możliwości udzielenia odpowiedzi. Proszę podać mi więcej informacji na temat tego, o czym chcesz porozmawiać lub zadaj konkretne pytanie, a będę mógł pomóc w rozwinięciu wypowiedzi.Boundless DAS differs from traditional alignment-based algorithms in several important ways. First, traditional alignment-based algorithms typically rely on pre-defined hypotheses and a fixed search space, whereas Boundless DAS can search an unbounded space of causal structures by iteratively adding nodes and edges to the graph until a stopping criterion is met. This means that Boundless DAS can explore a wider range of possible causal structures, which may be more relevant to the model being studied.\n\nSecond, traditional alignment-based algorithms can suffer from the curse of dimensionality, which makes them difficult to scale to models with a large number of parameters. In contrast, Boundless DAS is able to scale to models with billions of parameters by leveraging the sparsity of the causal graph and using a distributed computing framework to parallelize the search.\n\nFinally, traditional alignment-based algorithms often require a large amount of computational resources and a priori knowledge of the causal structure, whereas Boundless DAS is able to find structure even with suboptimal generation methods or assessment metrics. This makes it a powerful tool for exploring the causal structure of complex models, such as large language models.\n\nFurthermore, Boundless DAS uses gradient descent to learn an alignment between causal variables and fixed dimensionality linear subspaces of neural representations in a network. This method is able to capture more complex relationships between causal variables and neural representations, compared to traditional methods that often focus on zeroing-out representations or substituting with mean values. By incorporating both causal variables and neural representations in the alignment process, Boundless DAS is able to achieve better performance compared to traditional methods.\n\nIn summary, Boundless DAS represents a novel approach to alignment-based algorithms that offers several advantages over existing methods, including the ability to search an unbounded space of causal structures, scalability to models with billions of parameters, and the ability to find structure even with suboptimal generation methods or assessment metrics. This makes it a powerful tool for exploring the causal structure of complex models, such as large language models, and represents an important step forward in the development of interpretability methods for AI safety.Oczywiście, z przyjemnością rozwinę swoją wypowiedź, jeśli tylko podasz mi konkretne pytanie lub temat. Bez nich nie jestem w stanie zacząć dyskusji ani udzielić odpowiedzi. Proszę, podaj mi więcej informacji na temat tematu, który Cię interesuje, a ja postaram się odpowiedzieć na pytania tak szczegółowo, jak to możliwe.Distributed Alignment Search (DAS) and Boundless DAS are important methods used in Alpaca language models to achieve human-interpretable explanations of the model's behavior. These methods aim to uncover the causal mechanisms underlying the model's outputs and to generalize to new, unseen inputs. \n\nDAS is based on a theory of causal abstraction and uses gradient descent to learn perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. This method helps to identify the neural representations that align with the interpretable variables in the model, leading to a better understanding of how the model works. \n\nBoundless DAS is an extension of DAS that scales up the approach by replacing the remaining brute-force search steps with learned parameters. This allows for more efficient and effective interpretation of the Alpaca language model, which contains 7B parameters. By applying Boundless DAS to Alpaca, researchers were able to discover that the model implements a causal model with two interpretable boolean variables to solve a simple numerical reasoning problem. The alignment of neural representations with these variables was found to be robust to changes in inputs and instructions. \n\nOverall, DAS and Boundless DAS play a crucial role in helping researchers deeply understand the inner-workings of large language models like Alpaca. They are important for ensuring the interpretability and safety of AI models and for identifying the causal mechanisms underlying their behavior.Przykro mi, ale nie jestem w stanie rozwinąć wypowiedzi na temat podanego fragmentu tekstu, ponieważ nie ma wystarczającej ilości informacji, które można by wykorzystać do tego celu. Brakuje kontekstu i szczegółów, które pozwoliłyby na bardziej szczegółową interpretację tekstu i udzielenie bardziej wyczerpującej odpowiedzi. Potrzebne są dodatkowe informacje lub konkretne pytanie, aby móc zrozumieć, o co dokładnie chodzi i udzielić pełnej odpowiedzi.Unfortunately, none of the provided parts of the document give any information on how these technologies improve the accuracy of natural language processing tasks in Alpaca language models. The text only presents the results of experiments conducted to evaluate the accuracy of different alignment proposals, the use of Boundless DAS to search for interpretable causal structures in large language models, and the high-level causal models for price tagging tasks. However, it does not provide any information on how these findings or technologies directly enhance the accuracy of natural language processing tasks in Alpaca language models.\n\nTo answer this question, we need to look for additional information or studies that specifically investigate the effect of these technologies on the accuracy of NLP tasks in Alpaca language models. Without such information, we cannot provide a detailed answer to the question.Niestety, nie jestem w stanie rozwijać mojej wypowiedzi, ponieważ nie zostało podane żadne konkretne pytanie lub tematyka, na którą miałbym się skupić. W mojej wcześniejszej odpowiedzi wskazałem/am, że potrzebuję konkretnych informacji, aby móc odpowiedzieć na pytanie z jak największą ilością szczegółów. Jeśli masz konkretne pytanie lub temat, na który chcesz uzyskać odpowiedź, proszę o podanie go, a ja postaram się pomóc w miarę swoich możliwości.The given portion of the document does not provide a direct answer to whether Distributed Alignment Search (DAS) and Boundless DAS can be used in other types of NLP applications besides Alpaca language models. However, based on the general description of the Boundless DAS method in the text, it is possible that it can be applied to other types of NLP applications as well. \n\nThe document explains DAS as a method for learning an alignment between causal variables of a model and linear subspaces of neural representations in a network using gradient descent. The authors note that Boundless DAS is a more efficient version of DAS that can be used for scaling alignment search of causal structure in language models. \n\nThe text implies that Boundless DAS can potentially be applied to other language models besides Alpaca if they have smaller parameter sizes and require a full-rank square rotation matrix. However, the authors note that it is currently impossible to apply Boundless DAS to Alpaca, as aligning 10 token representations would result in a rotation matrix containing 1.6B parameters, which is too large. \n\nTherefore, it seems that the applicability of Boundless DAS depends on the size and architecture of the language model being used. Further research and experimentation would be necessary to determine the effectiveness and limitations of using Boundless DAS in other NLP contexts. Nonetheless, the authors suggest that once larger LLMs are released and evaluated as having stronger reasoning abilities, Boundless DAS will be ready as an analysis tool.Oczywiście, chętnie rozwinę moją wypowiedź o jak najwięcej szczegółów, jeśli tylko podasz mi konkretne pytanie lub temat, na którym się skupić. Bez konkretnego punktu wyjścia, nie jestem w stanie przekazać żadnej szczegółowej informacji. \n\nJeśli chodzi o ogólną wypowiedź, to mogę powiedzieć, że jestem ekspertem w swojej dziedzinie i posiadam wiedzę na wiele tematów. Mogę opowiedzieć o technologiach informatycznych, językach programowania, zagadnieniach związanych z internetem, oprogramowaniu i wielu innych. \n\nJeśli potrzebujesz pomocy w rozwiązaniu konkretnego problemu, to proszę podać szczegóły i postaram się pomóc w jak największym stopniu. W miarę możliwości, postaram się wykorzystać swoją wiedzę i doświadczenie, aby udzielić Ci dokładnej i wyczerpującej odpowiedzi.Nie wiadomo dokładnie, o jakie pytanie dotyczące tekstu chodzi, ale można opowiedzieć o treści wycinków, które zostały przedstawione.\n\nW pierwszym fragmencie tekstu autorzy opisują proces ewaluacji modelu Alpaca w wykonaniu zadania polegającego na szacowaniu wartości liczb. Model uzyskał wynik 85% w losowo wybranych parach liczb, co jest wynikiem dobrym, ale nie doskonałym. Autorzy zwracają uwagę na trudność znalezienia dobrego zadania wymagającego pośrednich etapów, które modele językowe o dużej pojemności potrafią rozwiązać. Przetestowano różne zadania z zakresu matematyki, logiki, pytań i odpowiedzi oraz kodowania, ale modele miały problemy z ich wykonaniem. W celu szkolenia modelu Boundless DAS, autorzy musieli wygenerować zestawy danych kontrfaktycznych, w których każdy przykład zawierał parę wejść jako bazę i źródło oraz wskaźnik interwencji.\n\nDrugi fragment tekstu przedstawia badania dotyczące mechanizmów działania dużych modeli językowych, w tym modelu Alpaca. Autorzy zastosowali technikę Boundless DAS w celu odkrycia interpretowalnych struktur przyczynowo-skutkowych w modelu. Okazało się, że Alpaca wykorzystuje model przyczynowy z dwoma interpretowalnymi zmiennymi boolowskimi do rozwiązywania prostych zadań z dziedziny matematyki. Odnaleziona przez badaczy struktura przyczynowa jest stabilna i niezależna od zmian w danych wejściowych czy instrukcjach. Te wyniki stanowią pierwszy krok w kierunku głębszego zrozumienia działania największych i najbardziej rozpowszechnionych modeli językowych.\n\nW skrócie, oba fragmenty tekstu opisują badania dotyczące modelu językowego Alpaca. Pierwszy fragment przedstawia wyniki ewaluacji modelu w wykonaniu zadania szacowania wartości liczb oraz problemy z wykonaniem innych zadań. Drugi fragment opisuje zastosowaną technikę Boundless DAS do odkrycia interpretowalnych struktur przyczynowo-skutkowych w modelu Alpaca i wyniki tych badań.Oczywiście, chętnie pomogę. Proszę podać temat, na którym chciałbyś uzyskać więcej informacji, a postaram się udzielić odpowiedzi w miarę możliwości i dostępności informacji na dany temat.Unfortunately, the given portion of the document does not provide a clear list of benefits of incorporating Distributed Alignment Search and Boundless DAS into Alpaca language models. However, it does mention some of the advantages of using Boundless DAS for scaling alignment search of causal structure in LLMs.\n\nOne of the main benefits of using Boundless DAS is that it allows for the scaling of alignment search of causal structure in LLMs to billions of parameters. This is important because it enables the models to perform complex cognitive tasks while maintaining interpretability.\n\nAnother advantage of using Boundless DAS is that it can find structure even where the model's task performance is low, since task performance can be shaped by factors like a suboptimal generation method or the rigidity of the assessment metric. This means that the model is able to learn more effectively and improve its performance over time.\n\nIn addition, using Boundless DAS, the authors of the document found that Alpaca, an off-the-shelf language model, was able to solve a simple numerical reasoning problem in a human-interpretable way. This suggests that incorporating Boundless DAS into Alpaca language models may enable them to perform more complex cognitive tasks while still being easily interpretable.\n\nHowever, the text also notes that even with Boundless DAS, the model may only find partial support for causal structure, depending on the quality of the hypotheses and the search method. Therefore, future work is needed to improve the connection between Boundless DAS and language model errors.Oczywiście, z przyjemnością przedstawię więcej informacji na temat tego fragmentu tekstu. Jak już wspomniałem, fragment odnosi się do artykułu naukowego, który prezentuje narzędzie pozwalające na znajdowanie składni w reprezentacjach słów. Niestety, nie ma w nim szczegółowych informacji na temat tego narzędzia ani jego zastosowań.\n\nJednakże, jeśli chodzi o znaczenie tego tematu, to warto zaznaczyć, że narzędzia do znajdowania składni w reprezentacjach słów są bardzo ważne w dziedzinie uczenia maszynowego i sztucznej inteligencji. Składnia jest jednym z podstawowych elementów języka naturalnego, a jej zrozumienie jest kluczowe dla takich zadań jak rozpoznawanie mowy, analiza tekstu czy tłumaczenie maszynowe.\n\nW praktyce, narzędzia do znajdowania składni w reprezentacjach słów są wykorzystywane w różnych aplikacjach, takich jak chatboty, inteligentne asystenty głosowe czy systemy rekomendacyjne. Dzięki nim, komputery są w stanie lepiej zrozumieć intencje użytkowników i udzielać bardziej precyzyjnych odpowiedzi.\n\nJeśli chodzi o konkretny artykuł, to warto zaznaczyć, że został on przedstawiony na konferencji ACM SIGKDD w 2019 roku we Florencji we Włoszech. SIGKDD to jedna z najważniejszych konferencji naukowych w dziedzinie wydobywania wiedzy z danych i uczenia maszynowego. To miejsce, gdzie prezentowane są najnowsze osiągnięcia i narzędzia w tej dziedzinie, a także omawiane są najważniejsze problemy i wyzwania.\n\nMam nadzieję, że ta rozwinięta odpowiedź na zadane pytanie jest bardziej satysfakcjonująca i pomogła w zrozumieniu tematu.I'm sorry, but there is no context or question provided for me to give a relevant answer. Can you please provide more information or a specific question?Oczywiście, chętnie rozwiniemy temat. Publikacja naukowa \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\" autorstwa Marco Tulio Ribeiro, Sameer Singh i Carlos Guestrin została przedstawiona podczas konferencji KDD'16. Artykuł ten dotyczy istotnego problemu w dziedzinie uczenia maszynowego, którym jest wyjaśnianie predykcji dowolnego klasyfikatora.\n\nW uczeniu maszynowym klasyfikacja opiera się na modelach, które uczą się na podstawie danych treningowych i stosowane są do przewidywania wyników dla nowych danych. Jednakże, podczas gdy skuteczność tych modeli jest często potwierdzana za pomocą metryk oceny jakości, jak np. dokładność, to zwykle trudno jest zrozumieć, dlaczego dany model dokonuje konkretnej predykcji. W przypadkach, gdy predykcje te mają znaczenie dla ludzi, jak np. w medycynie czy finansach, istnieje potrzeba zrozumienia, co wpłynęło na decyzję modelu. \n\nArtykuł Ribeiro, Singha i Guestrina proponuje rozwiązanie tego problemu poprzez opracowanie narzędzia, które pozwalałoby na łatwe i intuicyjne wyjaśnienie predykcji dowolnego klasyfikatora. W ramach artykułu badacze przedstawiają nową metodę wyjaśniania predykcji, którą nazwali \"LIME\" (Local Interpretable Model-Agnostic Explanations). Metoda ta polega na generowaniu lokalnych modeli interpretowalnych dla każdego punktu danych, na podstawie których można wyjaśnić predykcję klasyfikatora dla tego punktu.\n\nW praktyce, metoda LIME pozwala na proste i zrozumiałe wyjaśnienie, dlaczego dany klasyfikator dokonał konkretnej predykcji dla konkretnych danych. Może to mieć zastosowanie w wielu dziedzinach, np. w medycynie, gdzie lekarze mogliby zrozumieć, dlaczego dany model zaproponował konkretne leczenie dla pacjenta, lub w finansach, gdzie analitycy mogliby zrozumieć, dlaczego dany model zaproponował konkretną inwestycję dla klienta.\n\nW sumie, publikacja ta stanowi ważny krok w kierunku zrozumienia, jak działają modele uczenia maszynowego i w jaki sposób można je wykorzystać w praktyce.NLP (Natural Language Processing) is a field of artificial intelligence that focuses on enabling machines to understand human language. In recent years, there have been significant advances in NLP technology, resulting in increased accuracy in several natural language processing tasks such as sentiment analysis, text classification, and machine translation. These improvements have been driven by the development of more sophisticated algorithms, the availability of large datasets, and the emergence of deep learning techniques. Additionally, advancements in hardware such as GPUs have made it possible to process massive amounts of data quickly, which has contributed to the increase in accuracy. With this increased accuracy, NLP is becoming more useful in various applications, including chatbots, virtual assistants, and sentiment analysis tools. However, despite the progress, there are still challenges that need to be addressed, such as bias in language models and the need for more interpretability in NLP models.The ability to handle complex sentence structures and syntax is a crucial aspect of natural language processing. With the increasing complexity of natural language, there is a need for more sophisticated techniques that can handle various sentence structures and syntax effectively. \n\nOne potential approach to enhancing this ability is the use of structural probes, such as the method proposed by John Hewitt and Christopher D Manning in their 2019 paper. This approach involves identifying syntax in word representations, which can lead to a better understanding of how words are structured within a sentence. \n\nBy improving the ability to handle complex sentence structures and syntax, natural language processing systems can more accurately interpret and generate human-like language. This can have significant implications in various fields, including language translation, chatbots, and virtual assistants. However, it's worth noting that there may be limitations to these techniques, and further research is needed to fully understand their potential and applicability in practice.Niestety, żadna z podanych fragmentów tekstu nie zawiera informacji na temat poprawy wydajności na dużych zbiorach danych. Bez dodatkowych informacji na ten temat, nie jestem w stanie udzielić bardziej szczegółowej odpowiedzi. Możliwe jednak, że w dalszej części dokumentu znajdują się informacje na temat takiej poprawy.I'm sorry, but without any specific information or context to work with, I cannot provide a detailed answer about reduced computational costs and faster processing times. If you could provide more information or a specific section of the document, I would be happy to try and assist you further."
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chcesz sprawdzić swoją wiedzę?:tak\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a12e39c29fad>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mjezyk_literatury\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eng\"\u001b[0m \u001b[0;31m#@param {type:\"string\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mrun_compendium\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnazwa_kursu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzakres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliteratura\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjezyk_literatury\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mchat_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"menu\"\u001b[0m \u001b[0;31m# powrot do menu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-4d0fdef5190d>\u001b[0m in \u001b[0;36mrun_compendium\u001b[0;34m(nazwa_kursu, zakres, url, lang)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# QA Maker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chcesz sprawdzić swoją wiedzę?:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0mQA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_QA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0mcontent\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n## Test wiedzy:\\n{QA}\\n\"\u001b[0m \u001b[0;31m# BUG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mchat_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8de2aa194f68>\u001b[0m in \u001b[0;36mget_QA\u001b[0;34m(summary, questions_list)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mtesterChain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpromptTester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtesterChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"QA\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mquestions_list\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_only_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             outputs = (\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    141\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    142\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mllm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_llm_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mgenerations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[1;32m     81\u001b[0m         )\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             results = [\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             results = [\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager)\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mChatResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_combine_llm_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_outputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mis_explicit_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTryAgain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36m_completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mretry_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         )\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             return (\n\u001b[0;32m--> 624\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    625\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    688\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 5265 tokens. Please reduce the length of the messages."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title OpenAI api usage\n",
        "f\"Used {chat_config.used_tokens} tokens, what cost: ${chat_config.total_cost}\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xMczUL0DsTM5",
        "outputId": "780c52e1-f532-4939-f2a5-d737dac6588b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Used 726 tokens, what cost: $0.0014520000000000002'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_config.audio_state = \"off\""
      ],
      "metadata": {
        "id": "_DoVfwjomksf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "0.007402 - 0.005946"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzB-DxXKyLFa",
        "outputId": "0b096e15-38f4-4608-89d7-a6d66950c36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0014559999999999998"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1balbn-yWVGV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}